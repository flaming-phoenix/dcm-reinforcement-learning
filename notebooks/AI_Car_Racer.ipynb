{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fNAz8PqLJ1BU"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIcEJ-YwD606"
   },
   "source": [
    "# AI Car Racer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldpBz48oIiMx"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to the AI Car Racer competition! You're about to teach a car to race around a track using reinforcement learning (RL). This is a competition: **complete a full lap in the fastest time to win**.\n",
    "\n",
    "\n",
    "## What You're Building\n",
    "\n",
    "You'll train an AI agent using **Deep Q-Learning (DQN)** to control a car racing around a track. The model \"sees\" an image of the track, and learns to choose actions such as (turn left, turn right, accelerate, brake, or do nothing) to maximize its score.\n",
    "\n",
    "\n",
    "## Competition Rules\n",
    "\n",
    "- **Goal**: Complete a full lap around the track\n",
    "- **Winner**: Fastest lap time\n",
    "- **Track**: Fixed seed (everyone gets the same track)\n",
    "- **Starting point**: Train from scratch\n",
    "- **You can**: Tune any hyperparameters and modify wrappers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "6ZtWBcazJMsZ",
    "outputId": "bc59135f-7058-4c79-9dde-4b2d79e5937f"
   },
   "outputs": [],
   "source": [
    "Image(url='https://gymnasium.farama.org/_images/car_racing.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6WmFuSvGXtm"
   },
   "source": [
    "## Google Colab\n",
    "\n",
    "### What is Colab?\n",
    "\n",
    "Google Colab is like Google Docs for code—it's a free Jupyter notebook environment that runs in your browser.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Cells**: Blocks of code or text. Run them with `Shift + Enter` or click the ▶️ button\n",
    "- **Code cells**: Contain Python code you can execute\n",
    "- **Text cells**: Contain formatted text (like this README)\n",
    "- **Runtime**: The virtual computer running your code\n",
    "  - Go to `Runtime → Change runtime type` to select GPU\n",
    "  - Free GPUs speed up training significantly\n",
    "- **Session timeout**: After 12 hours or if idle, your runtime disconnects. Your code remains but variables reset\n",
    "\n",
    "### Essential Shortcuts\n",
    "\n",
    "- `Shift + Enter`: Run current cell and move to next\n",
    "- `Ctrl + Enter`: Run current cell and stay on it\n",
    "- `Ctrl + S`: Save notebook\n",
    "- `Ctrl + /`: Comment/uncomment code\n",
    "\n",
    "### Colab File System\n",
    "\n",
    "- Files you create live in `/content/` directory\n",
    "- **Important**: Files are temporary! They disappear when runtime disconnects\n",
    "- Download important files (models, logs) to your local machine\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HCQrzV_QqzT"
   },
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "-v7wY6ozQqZs",
    "outputId": "19091ba5-74f6-485a-f3bb-7c4af22f374d"
   },
   "outputs": [],
   "source": [
    "Image(url='https://media.geeksforgeeks.org/wp-content/uploads/20220214110501/ImagefromiOS1-660x296.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUEoz6ZlR83n"
   },
   "source": [
    "### Environment\n",
    "\n",
    "This is the game world and all of its components, think of the track, the car, the physics and how they all interact together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNjZQ7owR-tZ"
   },
   "source": [
    "### Observations (state)\n",
    "This is what the model can 'see'\n",
    "\n",
    "https://gymnasium.farama.org/environments/box2d/car_racing/#observation-space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFW0T7y2M-oo"
   },
   "source": [
    "### Actions\n",
    "Based on the observations the model will then pick the best action to take.\n",
    "\n",
    "**Continous**\n",
    "\n",
    "This is like having a real steering wheel, with a gas and brake pedal.\n",
    "\n",
    "| Index | Control  | Range        | Meaning                            |\n",
    "|-------|----------|--------------|------------------------------------|\n",
    "| 0     | Steering | [-1.0, +1.0] | -1 = full left, +1 = full right    |\n",
    "| 1     | Gas      | [0.0, 1.0]   | 0 = no throttle, 1 = full throttle |\n",
    "| 2     | Brake    | [0.0, 1.0]   | 0 = no brake, 1 = full brake       |\n",
    "\n",
    "\n",
    "_Example action_:\n",
    "[0.3, 0.8, 0.0] → \"Turn slightly right and 80% throttle, no brake.\"\n",
    "\n",
    "Note the `continuous=True`\n",
    "\n",
    "```python\n",
    "import gymnasium\n",
    "env = gymnasium.make(\"CarRacing-v3\", continuous=True)\n",
    "```\n",
    "\n",
    "**Discrete**\n",
    "\n",
    "\n",
    "Discrete actions are like buttons where the model chooses a button to press at each timestep.\n",
    "\n",
    "| Action | Meaning     |\n",
    "|--------|-------------|\n",
    "| 0      | Do nothing  |\n",
    "| 1      | Steer right |\n",
    "| 2      | Steer left  |\n",
    "| 3      | Gas         |\n",
    "| 4      | Brake       |\n",
    "\n",
    "Note the `continuous=False`\n",
    "\n",
    "```python\n",
    "import gymnasium\n",
    "env = gymnasium.make(\"CarRacing-v3\", continuous=False)\n",
    "```\n",
    "\n",
    "\n",
    "**Choosing Between Continuous and Discrete**\n",
    "\n",
    "| Mode           | Pros                      | Cons                            |\n",
    "|----------------|---------------------------|---------------------------------|\n",
    "| **Continuous** | Realistic, smooth control | Model will take longer to train |\n",
    "| **Discrete**   | Simple controls           | Quicker to learn, less control  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3PTe-WySBqP"
   },
   "source": [
    "### Reward\n",
    "\n",
    "The model is provided with a reward for each action it takes. The reward provides feedback to the model to determine if it should take more actions like that or less actions like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZ3YRWKzd_c2"
   },
   "source": [
    "## Hyperparameters Explained\n",
    "\n",
    "These are the knobs you can turn to improve performance. **This is where you'll win the competition!**\n",
    "\n",
    "In Python, `5e-4` is just a shorthand for writing `0.0005`. The `e-4` means 'shift the decimal 4 places left' or '5 times 10 to the power of -4'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDvBIP5FX1S7"
   },
   "source": [
    "### Core Training Parameters\n",
    "\n",
    "#### `total_timesteps` (Default: 500,000)\n",
    "**What it does**: Total number of actions the agent takes during training. Start small 10k and build up to see how your model reacts\n",
    "**Think of it as**: How many practice laps your car gets.  \n",
    "- **Lower** (10k): Faster training but may not learn completely\n",
    "- **Higher** (500k): Better final performance but takes longer\n",
    "\n",
    "#### `learning_rate` (Default: 1e-4)\n",
    "**What it does**: How big each update step is when learning.  \n",
    "**Think of it as**: How quickly the car adjusts its strategy after each mistake.  \n",
    "**Range to try**: 5e-5 to 5e-4\n",
    "- **Lower** (5e-5): More stable, slower learning, less likely to \"forget\"\n",
    "- **Higher** (5e-4): Faster learning but can be unstable, might overshoot\n",
    "- **Sweet spot**: 1e-4 is a solid default\n",
    "\n",
    "#### `gamma` (Default: 0.98)\n",
    "**What it does**: Discount factor for future rewards.  \n",
    "**Think of it as**: How much the car values long-term success vs immediate rewards.  \n",
    "**Range to try**: 0.95 - 0.995\n",
    "- **Lower** (0.95): Car focuses on immediate rewards, more aggressive\n",
    "- **Higher** (0.995): Car plans ahead more, smoother driving\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-8EpZ24d7sS"
   },
   "source": [
    "### Exploration Parameters\n",
    "\n",
    "#### `exploration_fraction` (Default: 0.3)\n",
    "**What it does**: What fraction of training to spend exploring randomly.  \n",
    "**Think of it as**: How long the car experiments before settling on a strategy.  \n",
    "**Range to try**: 0.2 - 0.5\n",
    "- **Lower** (0.2): Commits to learned strategy sooner\n",
    "- **Higher** (0.5): Explores longer, might find better solutions\n",
    "- **Sweet spot**: 0.3 for most cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7na25Uujkz88"
   },
   "source": [
    "## Experimentation Strategy\n",
    "\n",
    "### Phase 1: Quick Iteration (45 min)\n",
    "Try faster training runs to test ideas:\n",
    "- Reduce `total_timesteps` to 50k for quick tests\n",
    "- Try 2-3 different hyperparameter combinations\n",
    "- Focus on `learning_rate`, `gamma`, and exploration parameters\n",
    "\n",
    "### Phase 2: Final Training (60 min)\n",
    "Once you find promising settings, do a full training run\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVFt5TWQlWHL"
   },
   "source": [
    "## Monitoring Training\n",
    "\n",
    "### Watch the Logs\n",
    "\n",
    "Key metrics to monitor in the training output:\n",
    "\n",
    "```python\n",
    "# After training starts, you'll see:\n",
    "ep_rew_mean: -50 → -20 → 100 → 300 → 500+  # Getting better!\n",
    "ep_len_mean: 50 → 100 → 200 → 500+          # Driving for longer!\n",
    "```\n",
    "\n",
    "**Good signs:**\n",
    "- `ep_rew_mean` increasing over time\n",
    "- `ep_len_mean` increasing (car survives longer)\n",
    "- Fewer negative rewards\n",
    "\n",
    "**Bad signs:**\n",
    "- `ep_rew_mean` stuck or decreasing\n",
    "- Very short episodes throughout training\n",
    "- Loss values exploding (> 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Lv8Kl_VoNxL"
   },
   "source": [
    "# Configuration\n",
    "\n",
    "Edit these variables to customise your training.  All the main settings are right here, though the hyperparameters for different training schemes can differ.\n",
    "\n",
    "## ALGORITHM SELECTION\n",
    "\n",
    "Choose which reinforcement learning algorithm to use.\n",
    "\n",
    " Options:\n",
    "   - \"DQN\"  : Deep Q-Network (works with DISCRETE actions only)\n",
    "             Good for: Simple action spaces, faster training on discrete problems\n",
    "             Note: DQN ONLY supports discrete actions (like pressing buttons)\n",
    "\n",
    "   - \"PPO\"  : Proximal Policy Optimization (works with BOTH discrete AND continuous)\n",
    "             Good for: More complex tasks, smoother learning, works with continuous control\n",
    "             Note: PPO is more flexible and can handle both action types\n",
    "\n",
    " For more algorithms, see: https://stable-baselines3.readthedocs.io/en/master/guide/algos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHM = \"DQN\"  # Options: \"DQN\" or \"PPO\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Lv8Kl_VoNxL"
   },
   "source": [
    "# Action Type\n",
    "\n",
    "This setting is ONLY relevant if you choose PPO above.\n",
    " DQN always uses discrete actions (this setting is ignored for DQN).\n",
    "\n",
    " Options:\n",
    "   - \"discrete\"   : Button-like controls (left, right, gas, brake, nothing)\n",
    "                    Simpler for the model to learn, but less precise control\n",
    "\n",
    "   - \"continuous\" : Analog controls (like a real steering wheel + pedals)\n",
    "                    More precise control, but takes longer to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_ACTION_SPACE = \"discrete\"  # Options: \"discrete\" or \"continuous\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Lv8Kl_VoNxL"
   },
   "source": [
    "## Checkpoint settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_FREQUENCY = 10000  # Save model every X steps\n",
    "CHECKPOINT_DIR = \"./checkpoints/\"  # Folder to save checkpoints\n",
    "RESUME_TRAINING = True  # Set to True to continue from a checkpoint, False to start a new model\n",
    "CHECKPOINT_PATH = \"./checkpoints/best_model.zip\"  # Path to load checkpoint from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Lv8Kl_VoNxL"
   },
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_TIMESTEPS = 50_000   # Total training steps (start with 50k for testing)\n",
    "LEARNING_RATE = 5e-4        # How fast the model learns (try: 5e-5 to 5e-4)\n",
    "GAMMA = 0.98                # Future reward discount (try: 0.95 to 0.995)\n",
    "\n",
    "# DQN-specific parameters (ignored when using PPO)\n",
    "EXPLORATION_FRACTION = 0.3  # Fraction of training spent exploring randomly\n",
    "BUFFER_SIZE = 100_000       # Size of replay buffer (memory for past experiences)\n",
    "BATCH_SIZE = 64             # Number of samples per training update\n",
    "\n",
    "# PPO-specific parameters (ignored when using DQN)\n",
    "N_STEPS = 2048              # Steps to collect before each update\n",
    "N_EPOCHS = 10               # Number of epochs when updating\n",
    "CLIP_RANGE = 0.2            # PPO clipping parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Lv8Kl_VoNxL"
   },
   "source": [
    "## Logging and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = \"./logs/\"         # TensorBoard logs directory\n",
    "EVAL_FREQ = 10000           # Evaluate model every X steps\n",
    "N_EVAL_EPISODES = 5         # Number of episodes for each evaluation\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"   Algorithm: {ALGORITHM}\")\n",
    "if ALGORITHM == \"PPO\":\n",
    "    print(f\"   Action Space: {PPO_ACTION_SPACE}\")\n",
    "else:\n",
    "    print(f\"   Action Space: discrete (DQN only supports discrete)\")\n",
    "print(f\"   Resume Training: {RESUME_TRAINING}\")\n",
    "print(f\"   Checkpoint Frequency: Every {CHECKPOINT_FREQUENCY:,} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lAtwpXE_D6op",
    "outputId": "735a281b-dd60-4470-e7e9-2de1c3797d95"
   },
   "outputs": [],
   "source": [
    "!pip install \"swig>=4.3.1.post0\"\n",
    "!pip install \"gymnasium[box2d]==1.2.0\"\n",
    "!pip install \"stable-baselines3[extra]==2.7.0\"\n",
    "!pip install \"pyvirtualdisplay\"\n",
    "!sudo apt-get install -y xvfb ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8xKhv_NxOow"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import gymnasium\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import ResizeObservation\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv  # Environment wrapper\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from gymnasium.wrappers import TransformAction\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "track_seed = 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHhco8OZDAyS"
   },
   "outputs": [],
   "source": [
    "if ALGORITHM == \"DQN\":\n",
    "    use_continuous = False\n",
    "    print(\"Using DISCRETE action space (required for DQN)\")\n",
    "    print(\"   Actions: [0] Nothing, [1] Right, [2] Left, [3] Gas, [4] Brake\")\n",
    "elif ALGORITHM == \"PPO\":\n",
    "    use_continuous = (PPO_ACTION_SPACE == \"continuous\")\n",
    "    if use_continuous:\n",
    "        print(\"Using CONTINUOUS action space (PPO)\")\n",
    "        print(\"   Actions: Steering [-1,+1], Gas [0,1], Brake [0,1]\")\n",
    "    else:\n",
    "        print(\"Using DISCRETE action space (PPO)\")\n",
    "        print(\"   Actions: [0] Nothing, [1] Right, [2] Left, [3] Gas, [4] Brake\")\n",
    "else:\n",
    "    raise ValueError(f\"Unknown algorithm: {ALGORITHM}. Use 'DQN' or 'PPO'.\")\n",
    "\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"\\nLog directory: {LOG_DIR}\")\n",
    "print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "\n",
    "def make_env(continuous: bool):\n",
    "    \"\"\"\n",
    "    Create a Car Racing environment with proper wrappers.\n",
    "    \n",
    "    Args:\n",
    "        continuous: If True, use continuous (analog) controls.\n",
    "                   If False, use discrete (button) controls.\n",
    "    \n",
    "    Returns:\n",
    "        A wrapped gymnasium environment ready for training.\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        # Create the base Car Racing environment\n",
    "        # render_mode=\"rgb_array\" means we get pixel data (for training)\n",
    "        # Use render_mode=\"human\" if you want to watch it train (slower!)\n",
    "        env = gym.make(\n",
    "            \"CarRacing-v3\",\n",
    "            continuous=use_continuous,\n",
    "            render_mode=\"rgb_array\"\n",
    "        )\n",
    "        \n",
    "        env.reset(seed=track_seed)\n",
    "        \n",
    "        # Wrap with Monitor to record episode statistics (rewards, lengths)\n",
    "        # This data is used for logging and evaluation\n",
    "        env = Monitor(env, LOG_DIR)\n",
    "        \n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "env = DummyVecEnv([make_env(use_continuous)])\n",
    "\n",
    "print(f\"\\nEnvironment created successfully!\")\n",
    "print(f\"   Environment: CarRacing-v3\")\n",
    "print(f\"   Continuous: {use_continuous}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Lv8Kl_VoNxL"
   },
   "source": [
    "## Create or Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BRBB7RiTtec",
    "outputId": "4863aaa7-eb36-4d02-d27b-7e3aa19c774c"
   },
   "outputs": [],
   "source": [
    "if RESUME_TRAINING:\n",
    "    print(f\"Loading model from checkpoint: {CHECKPOINT_PATH}\")\n",
    "    if not os.path.exists(CHECKPOINT_PATH) and not os.path.exists(CHECKPOINT_PATH + \".zip\"):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Checkpoint not found: {CHECKPOINT_PATH}\\n\"\n",
    "            f\"   Make sure the file exists, or set RESUME_TRAINING = False to start fresh.\"\n",
    "        )\n",
    "    if ALGORITHM == \"DQN\":\n",
    "        model = DQN.load(CHECKPOINT_PATH, env=env)\n",
    "    elif ALGORITHM == \"PPO\":\n",
    "        model = PPO.load(CHECKPOINT_PATH, env=env)\n",
    "    \n",
    "    print(f\"Model loaded successfully!\")\n",
    "    print(f\"   You can now continue training from where you left off.\")\n",
    "    \n",
    "else:\n",
    "    # ─────────────────────────────────────────────────────────────────────────────\n",
    "    # CREATE A NEW MODEL\n",
    "    # ─────────────────────────────────────────────────────────────────────────────\n",
    "    # Starting fresh with a new, untrained model.\n",
    "    # The model will learn from scratch.\n",
    "    \n",
    "    print(f\"Creating a new {ALGORITHM} model...\")\n",
    "    \n",
    "    if ALGORITHM == \"DQN\":\n",
    "        \n",
    "        model = DQN(\n",
    "            policy=\"CnnPolicy\",\n",
    "            env=env,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            gamma=GAMMA,\n",
    "            buffer_size=BUFFER_SIZE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            exploration_fraction=EXPLORATION_FRACTION,\n",
    "            tensorboard_log=LOG_DIR,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "    elif ALGORITHM == \"PPO\":       \n",
    "        model = PPO(\n",
    "            policy=\"CnnPolicy\",\n",
    "            env=env,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            gamma=GAMMA,\n",
    "            n_steps=N_STEPS,\n",
    "            n_epochs=N_EPOCHS,\n",
    "            clip_range=CLIP_RANGE,\n",
    "            tensorboard_log=LOG_DIR,\n",
    "            verbose=1\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nNew {ALGORITHM} model created successfully!\")\n",
    "    print(f\"   Policy: CnnPolicy (Convolutional Neural Network)\")\n",
    "    print(f\"   Learning Rate: {LEARNING_RATE}\")\n",
    "    print(f\"   Gamma: {GAMMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Lv8Kl_VoNxL"
   },
   "source": [
    "## Set Up Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BRBB7RiTtec",
    "outputId": "4863aaa7-eb36-4d02-d27b-7e3aa19c774c"
   },
   "outputs": [],
   "source": [
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=CHECKPOINT_FREQUENCY,\n",
    "    save_path=CHECKPOINT_DIR,\n",
    "    name_prefix=\"rl_model\",\n",
    "    save_replay_buffer=True,\n",
    "    save_vecnormalize=True\n",
    ")\n",
    "\n",
    "print(f\"Checkpoint callback created\")\n",
    "print(f\"   Saving every {CHECKPOINT_FREQUENCY:,} steps to {CHECKPOINT_DIR}\")\n",
    "\n",
    "eval_env = DummyVecEnv([make_env(use_continuous)])\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=CHECKPOINT_DIR,\n",
    "    log_path=LOG_DIR,\n",
    "    eval_freq=EVAL_FREQ,\n",
    "    n_eval_episodes=N_EVAL_EPISODES,\n",
    "    render=False,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "print(f\"Evaluation callback created\")\n",
    "print(f\"   Evaluating every {EVAL_FREQ:,} steps\")\n",
    "print(f\"   Running {N_EVAL_EPISODES} episodes per evaluation\")\n",
    "print(f\"   Best model will be saved to {CHECKPOINT_DIR}best_model.zip\")\n",
    "\n",
    "# Combine all callbacks into a list\n",
    "callbacks = [checkpoint_callback, eval_callback]\n",
    "\n",
    "print(f\"\\nAll callbacks ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzGMKB5zljCa"
   },
   "source": [
    "## Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "fY9iY9z3Tlfd",
    "outputId": "91a8d1cb-783b-461b-eec3-1a0d06c9ec70"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(f\"STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Algorithm: {ALGORITHM}\")\n",
    "print(f\"Total Timesteps: {TOTAL_TIMESTEPS:,}\")\n",
    "print(f\"Checkpoints: Every {CHECKPOINT_FREQUENCY:,} steps\")\n",
    "print(f\"Evaluation: Every {EVAL_FREQ:,} steps\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTraining progress will be shown below...\")\n",
    "print(\"   Look for 'ep_rew_mean' to see if the car is improving!\")\n",
    "print(\"   Higher values = better driving\\n\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────────\n",
    "# THE MAIN TRAINING CALL\n",
    "# ─────────────────────────────────────────────────────────────────────────────────\n",
    "# This single line does ALL the training. The model.learn() method:\n",
    "#   1. Collects experiences by running the agent in the environment\n",
    "#   2. Updates the neural network based on those experiences\n",
    "#   3. Repeats for the specified number of timesteps\n",
    "#\n",
    "# The callbacks run periodically during training to save checkpoints and evaluate.\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=TOTAL_TIMESTEPS,\n",
    "    callback=callbacks,\n",
    "    progress_bar=True  # Show a nice progress bar\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjg1HY99pfe_"
   },
   "source": [
    "# How good is my model ?\n",
    "\n",
    "This will check that the model you have trained, over 10 laps and calculate the reward.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3GoJ2VGy0qrK",
    "outputId": "990ced16-ebb8-4b95-ab9a-ace0e22f25b1"
   },
   "outputs": [],
   "source": [
    "check_env = gymnasium.make(\"CarRacing-v3\",  render_mode='rgb_array', continuous=use_continuous)\n",
    "check_env = Monitor(check_env)\n",
    "mean_reward, std_reward = evaluate_policy(model, check_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjg1HY99pfe_"
   },
   "source": [
    "# Save the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3GoJ2VGy0qrK",
    "outputId": "990ced16-ebb8-4b95-ab9a-ace0e22f25b1"
   },
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name = f\"{timestamp}_{ALGORITHM}_CarRacingv3\"\n",
    "model_path = f\"./models/{model_name}\"\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "model.save(model_path)\n",
    "\n",
    "print(f\"Model saved successfully!\")\n",
    "print(f\"   Path: {model_path}.zip\")\n",
    "print(f\"\\nFor competition submission:\")\n",
    "print(f\"   1. Download the file: {model_path}.zip\")\n",
    "print(f\"   2. Note your settings:\")\n",
    "print(f\"      - Algorithm: {ALGORITHM}\")\n",
    "print(f\"Your submission must consist of the ZIP file containing the model and the Jupyter notebook you used\")\n",
    "if ALGORITHM == \"PPO\":\n",
    "    print(f\"      - Action Space: {PPO_ACTION_SPACE}\")\n",
    "else:\n",
    "    print(f\"      - Action Space: discrete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lphQ-TJJW-F2"
   },
   "source": [
    "# See your model drive round the track\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "2fkgmR8M0umO",
    "outputId": "48552ac3-6c4c-49c6-e171-1e510a06cd1b"
   },
   "outputs": [],
   "source": [
    "# Start virtual display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "# Setup the wrapper to record the video\n",
    "video_callable=lambda episode_id: True\n",
    "check_env = RecordVideo(check_env, video_folder='./videos', episode_trigger=video_callable)\n",
    "# If using PPO with continuous action space, wrap the environment\n",
    "if ALGORITHM == \"PPO\" and PPO_ACTION_SPACE == \"continuous\":\n",
    "    check_env = TransformAction(\n",
    "        check_env,\n",
    "        lambda action: np.array([\n",
    "            action[0],           # Steering: -1 to 1\n",
    "            action[1],           # Gas: 0 to 1\n",
    "            action[2]            # Brake: 0 to 1\n",
    "        ]),\n",
    "        check_env.action_space\n",
    "    )\n",
    "obs, info = check_env.reset()\n",
    "\n",
    "# Run the environment until done\n",
    "terminated = False\n",
    "truncated = False\n",
    "while not (terminated or truncated):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = check_env.step(action)\n",
    "\n",
    "check_env.close()\n",
    "\n",
    "# Display the video\n",
    "video = io.open(glob.glob('videos/*.mp4')[0], 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "ipythondisplay.display(HTML(data='''\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "    </video>\n",
    "'''.format(encoded.decode('ascii'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
